{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86aa81c",
   "metadata": {},
   "source": [
    "Embeddings with RNN (GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "5b039544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "bb1a6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_dataset_csv=\"train.csv\",\n",
    "    test_dataset_csv=\"test.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.3,\n",
    "    train_split_csv=\"train_with_splits.csv\", \n",
    "    #train_split_csv=\"train_tweet.csv\", \n",
    "    glove_patch='C:/Users/Admin/Desktop/NLP/embed/glove.6B.100d.txt',\n",
    "    seed=123)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "64686e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9815</td>\n",
       "      <td>trauma</td>\n",
       "      <td>LOCAL ATLANTA NEWS 4/28/00 - 4/28/15 FREELANCER</td>\n",
       "      <td>@raabchar_9 @drphil @morganlawgrp how do you s...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4527</td>\n",
       "      <td>emergency</td>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>anchorage jobs emergency medicine - nurse prac...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6540</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joboozoso :  usat usatoday_nfl michael floyd '...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1456</td>\n",
       "      <td>body bagging</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wwe 9k9 mycareer ep9 tyrone body bagging dudes...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10566</td>\n",
       "      <td>windstorm</td>\n",
       "      <td>she/her/your majesty/empress</td>\n",
       "      <td>i like the weird ones like rain of mystical or...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>4742</td>\n",
       "      <td>evacuate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sooo police dispatch said there was a person t...</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7479</th>\n",
       "      <td>9206</td>\n",
       "      <td>suicide bombing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suicide bombing is just the fear of dying alone</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>2540</td>\n",
       "      <td>collision</td>\n",
       "      <td>North Highlands, CA</td>\n",
       "      <td>traffic collision - ambulance enroute :  elkho...</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7481</th>\n",
       "      <td>4747</td>\n",
       "      <td>evacuate</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>@ahhtheenikki and from what i can tell- they r...</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7482</th>\n",
       "      <td>8172</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>17th Dimension</td>\n",
       "      <td>ah-mazing story of the power animal rescuers h...</td>\n",
       "      <td>1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7483 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          keyword                                         location  \\\n",
       "0      9815           trauma  LOCAL ATLANTA NEWS 4/28/00 - 4/28/15 FREELANCER   \n",
       "1      4527        emergency                                    Anchorage, AK   \n",
       "2      6540           injury                                              NaN   \n",
       "3      1456     body bagging                                              NaN   \n",
       "4     10566        windstorm                     she/her/your majesty/empress   \n",
       "...     ...              ...                                              ...   \n",
       "7478   4742         evacuate                                              NaN   \n",
       "7479   9206  suicide bombing                                              NaN   \n",
       "7480   2540        collision                              North Highlands, CA   \n",
       "7481   4747         evacuate                                        Nashville   \n",
       "7482   8172         rescuers                                   17th Dimension   \n",
       "\n",
       "                                                   text  target  split  \n",
       "0     @raabchar_9 @drphil @morganlawgrp how do you s...       0  train  \n",
       "1     anchorage jobs emergency medicine - nurse prac...       0  train  \n",
       "2     joboozoso :  usat usatoday_nfl michael floyd '...       0  train  \n",
       "3     wwe 9k9 mycareer ep9 tyrone body bagging dudes...       0  train  \n",
       "4     i like the weird ones like rain of mystical or...       0  train  \n",
       "...                                                 ...     ...    ...  \n",
       "7478  sooo police dispatch said there was a person t...       1    val  \n",
       "7479    suicide bombing is just the fear of dying alone       1    val  \n",
       "7480  traffic collision - ambulance enroute :  elkho...       1    val  \n",
       "7481  @ahhtheenikki and from what i can tell- they r...       1    val  \n",
       "7482  ah-mazing story of the power animal rescuers h...       1    val  \n",
       "\n",
       "[7483 rows x 6 columns]"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_splits=pd.read_csv(args.train_split_csv)\n",
    "train_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "a4556e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits['keyword']=train_splits.keyword.fillna('_nan_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "e408bcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5251 2232\n"
     ]
    }
   ],
   "source": [
    "train=train_splits[train_splits.split=='train']\n",
    "val=train_splits[train_splits.split=='val']\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbe6d3",
   "metadata": {},
   "source": [
    "The Vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "0912d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71391c",
   "metadata": {},
   "source": [
    "For text: Adding tokens of the beginning, end, unknown word, and the phrase complement symbol to the maximum length (mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "e13af5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            'unk_index' needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe015dc3",
   "metadata": {},
   "source": [
    "The Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "f4f546f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, text_vocab, keyword_vocab):\n",
    "        self.text_vocab = text_vocab\n",
    "        self.keyword_vocab = keyword_vocab\n",
    "        \n",
    "    def vectorize_text(self, text, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized text (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = [self.text_vocab.begin_seq_index]\n",
    "        for token in text.split(\" \"):\n",
    "            if (token not in string.punctuation):\n",
    "                indices.append(self.text_vocab.lookup_token(token))\n",
    "                \n",
    "        indices.append(self.text_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.text_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "    \n",
    "    def vectorize_keyword(self, keyword):\n",
    "        indices=[]\n",
    "        for word in keyword.split(' '):\n",
    "            if (word not in string.punctuation):\n",
    "                indices.append(self.keyword_vocab.lookup_token(word))\n",
    "                \n",
    "        out_vector=np.zeros(3, dtype=np.int64)\n",
    "        out_vector[:len(indices)]=indices  \n",
    "        \n",
    "        return out_vector\n",
    "     \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, tweet_df, cutoff=3):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            tweet_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the TweetVectorizer\n",
    "        \"\"\"\n",
    "        keyword_vocab = Vocabulary()        \n",
    "        for keyword in tweet_df.keyword:\n",
    "            for word in keyword.split(' '):\n",
    "                keyword_vocab.add_token(word)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for text in tweet_df.text:            \n",
    "            for token in text.split(\" \"):\n",
    "                if (token not in string.punctuation):\n",
    "                    #token_clear = re.sub(r\"([\\W9]+)\", r\"*\", token)\n",
    "                    #word_counts[token_clear] += 1\n",
    "                     word_counts[token] += 1\n",
    "        \n",
    "        text_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                text_vocab.add_token(word)\n",
    "        \n",
    "        return cls(text_vocab, keyword_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        text_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['text_vocab'])\n",
    "        keyword_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['keyword_vocab'])\n",
    "\n",
    "        return cls(text_vocab=text_vocab, keyword_vocab=keyword_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'text_vocab': self.text_vocab.to_serializable(),\n",
    "                'keyword_vocab': self.keyword_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "e92c8396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_vocab': {'token_to_idx': {'<MASK>': 0,\n",
       "   '<UNK>': 1,\n",
       "   '<BEGIN>': 2,\n",
       "   '<END>': 3,\n",
       "   'how': 4,\n",
       "   'do': 5,\n",
       "   'you': 6,\n",
       "   'a': 7,\n",
       "   'wound': 8,\n",
       "   'to': 9,\n",
       "   'your': 10,\n",
       "   'side': 11,\n",
       "   'and': 12,\n",
       "   'force': 13,\n",
       "   'trauma': 14,\n",
       "   'not': 15,\n",
       "   'with': 16,\n",
       "   'fall': 17,\n",
       "   'anchorage': 18,\n",
       "   'jobs': 19,\n",
       "   'emergency': 20,\n",
       "   'nurse': 21,\n",
       "   'healthcare': 22,\n",
       "   'ak': 23,\n",
       "   'em': 24,\n",
       "   'http': 25,\n",
       "   't': 26,\n",
       "   'co': 27,\n",
       "   'michael': 28,\n",
       "   'floyd': 29,\n",
       "   's': 30,\n",
       "   'hand': 31,\n",
       "   'injury': 32,\n",
       "   'shouldn': 33,\n",
       "   'devalue': 34,\n",
       "   'his': 35,\n",
       "   'fantasy': 36,\n",
       "   'stock': 37,\n",
       "   '9k9': 38,\n",
       "   'ep9': 39,\n",
       "   'body': 40,\n",
       "   'bagging': 41,\n",
       "   'dudes': 42,\n",
       "   'via': 43,\n",
       "   '@youtube': 44,\n",
       "   'i': 45,\n",
       "   'like': 46,\n",
       "   'the': 47,\n",
       "   'weird': 48,\n",
       "   'ones': 49,\n",
       "   'rain': 50,\n",
       "   'of': 51,\n",
       "   'or': 52,\n",
       "   'windstorm': 53,\n",
       "   'ocean': 54,\n",
       "   'waves': 55,\n",
       "   'https': 56,\n",
       "   'love': 57,\n",
       "   'amp': 58,\n",
       "   'can': 59,\n",
       "   'wait': 60,\n",
       "   'until': 61,\n",
       "   'collide': 62,\n",
       "   'm': 63,\n",
       "   'my': 64,\n",
       "   'own': 65,\n",
       "   'woman': 66,\n",
       "   'crush': 67,\n",
       "   'ignition': 68,\n",
       "   'knock': 69,\n",
       "   'detonation': 70,\n",
       "   'sensor': 71,\n",
       "   'connector-connecto': 72,\n",
       "   '9-9': 73,\n",
       "   'is': 74,\n",
       "   'it': 75,\n",
       "   'up': 76,\n",
       "   'steam': 77,\n",
       "   'roller': 78,\n",
       "   'object': 79,\n",
       "   'whether': 80,\n",
       "   'will': 81,\n",
       "   'be': 82,\n",
       "   'flattened': 83,\n",
       "   'better': 84,\n",
       "   'than': 85,\n",
       "   'tornado': 86,\n",
       "   'on': 87,\n",
       "   'town': 88,\n",
       "   'salem': 89,\n",
       "   'just': 90,\n",
       "   'ice': 91,\n",
       "   'bc': 92,\n",
       "   'im': 93,\n",
       "   'arsonist': 94,\n",
       "   'd': 95,\n",
       "   'wanted': 96,\n",
       "   'real': 97,\n",
       "   'casualty': 98,\n",
       "   'photos': 99,\n",
       "   'at': 100,\n",
       "   'tag': 101,\n",
       "   'us': 102,\n",
       "   'waiting': 103,\n",
       "   'for': 104,\n",
       "   'chocolate': 105,\n",
       "   'lava': 106,\n",
       "   'cakes': 107,\n",
       "   'get': 108,\n",
       "   'here': 109,\n",
       "   'yeah': 110,\n",
       "   '9': 111,\n",
       "   'god': 112,\n",
       "   'rock': 113,\n",
       "   'am': 114,\n",
       "   'ring': 115,\n",
       "   'desolation': 116,\n",
       "   'hd': 117,\n",
       "   'because': 118,\n",
       "   'need': 119,\n",
       "   'know': 120,\n",
       "   'if': 121,\n",
       "   'supposed': 122,\n",
       "   'throw': 123,\n",
       "   'myself': 124,\n",
       "   'off': 125,\n",
       "   'bridge': 126,\n",
       "   'collapse': 127,\n",
       "   'plan': 128,\n",
       "   'there': 129,\n",
       "   'no': 130,\n",
       "   'both': 131,\n",
       "   'words': 132,\n",
       "   'physical': 133,\n",
       "   'ripped': 134,\n",
       "   'apart': 135,\n",
       "   'while': 136,\n",
       "   'screamed': 137,\n",
       "   'dear': 138,\n",
       "   'been': 139,\n",
       "   'engulfed': 140,\n",
       "   'nothing': 141,\n",
       "   'wrong': 142,\n",
       "   'that': 143,\n",
       "   'lethal': 144,\n",
       "   'weapon': 145,\n",
       "   'series': 146,\n",
       "   'great': 147,\n",
       "   'yes': 148,\n",
       "   'they': 149,\n",
       "   're': 150,\n",
       "   'all': 151,\n",
       "   'finally': 152,\n",
       "   'girl': 153,\n",
       "   'want': 154,\n",
       "   'flooding': 155,\n",
       "   'yall': 156,\n",
       "   'wit': 157,\n",
       "   'pics': 158,\n",
       "   'rs': 159,\n",
       "   'tho': 160,\n",
       "   'panicking': 161,\n",
       "   'maybe': 162,\n",
       "   'bff': 163,\n",
       "   'left': 164,\n",
       "   'me': 165,\n",
       "   'china': 166,\n",
       "   'bar': 167,\n",
       "   'toilet': 168,\n",
       "   'horrible': 169,\n",
       "   'sinking': 170,\n",
       "   'feeling': 171,\n",
       "   'when': 172,\n",
       "   'you\\x89ûªve': 173,\n",
       "   'home': 174,\n",
       "   'phone': 175,\n",
       "   'realise': 176,\n",
       "   'its': 177,\n",
       "   '9g': 178,\n",
       "   'this': 179,\n",
       "   'whole': 180,\n",
       "   'time': 181,\n",
       "   'obliterated': 182,\n",
       "   'screen': 183,\n",
       "   'today': 184,\n",
       "   'drum': 185,\n",
       "   'stick': 186,\n",
       "   'blessed': 187,\n",
       "   'russian': 188,\n",
       "   'destroyed': 189,\n",
       "   'total': 190,\n",
       "   'tons': 191,\n",
       "   'food': 192,\n",
       "   'some': 193,\n",
       "   'italian': 194,\n",
       "   'were': 195,\n",
       "   'burned': 196,\n",
       "   'in': 197,\n",
       "   'an': 198,\n",
       "   'airport': 199,\n",
       "   'new': 200,\n",
       "   'ladies': 201,\n",
       "   'shoulder': 202,\n",
       "   'tote': 203,\n",
       "   'handbag': 204,\n",
       "   'women': 205,\n",
       "   'cross': 206,\n",
       "   'bag': 207,\n",
       "   'faux': 208,\n",
       "   'leather': 209,\n",
       "   'fashion': 210,\n",
       "   'purse': 211,\n",
       "   'full': 212,\n",
       "   're\\x89û_': 213,\n",
       "   'easy': 214,\n",
       "   'way': 215,\n",
       "   'look': 216,\n",
       "   'good': 217,\n",
       "   'after': 218,\n",
       "   'ray': 219,\n",
       "   'blew': 220,\n",
       "   'take': 221,\n",
       "   'demolished': 222,\n",
       "   'sent': 223,\n",
       "   'back': 224,\n",
       "   'fucking': 225,\n",
       "   'stone': 226,\n",
       "   'age': 227,\n",
       "   'wants': 228,\n",
       "   'become': 229,\n",
       "   'mass': 230,\n",
       "   'murderer': 231,\n",
       "   'many': 232,\n",
       "   'have': 233,\n",
       "   'deal': 234,\n",
       "   'daily': 235,\n",
       "   'haha': 236,\n",
       "   'traumatised': 237,\n",
       "   'hell': 238,\n",
       "   'job': 239,\n",
       "   'xxx': 240,\n",
       "   'guys': 241,\n",
       "   'these': 242,\n",
       "   'bitches': 243,\n",
       "   'ain': 244,\n",
       "   'famine': 245,\n",
       "   'then': 246,\n",
       "   'never': 247,\n",
       "   'watched': 248,\n",
       "   'pres': 249,\n",
       "   'think': 250,\n",
       "   'hes': 251,\n",
       "   'dick': 252,\n",
       "   'thunder': 253,\n",
       "   'though': 254,\n",
       "   'attacked': 255,\n",
       "   'friend': 256,\n",
       "   'came': 257,\n",
       "   'school': 258,\n",
       "   'asked': 259,\n",
       "   'him': 260,\n",
       "   'he': 261,\n",
       "   'was': 262,\n",
       "   'high': 263,\n",
       "   'said': 264,\n",
       "   'collapsed': 265,\n",
       "   'city': 266,\n",
       "   'bomb': 267,\n",
       "   'ass': 268,\n",
       "   'year': 269,\n",
       "   'burning': 270,\n",
       "   'buildings': 271,\n",
       "   'louis': 272,\n",
       "   'vuitton': 273,\n",
       "   'monogram': 274,\n",
       "   'sophie': 275,\n",
       "   'limited': 276,\n",
       "   'edition': 277,\n",
       "   'clutch': 278,\n",
       "   'read': 279,\n",
       "   'by': 280,\n",
       "   'ebay': 281,\n",
       "   'story': 282,\n",
       "   'sometimes': 283,\n",
       "   'face': 284,\n",
       "   'doing': 285,\n",
       "   'something': 286,\n",
       "   'but': 287,\n",
       "   'right': 288,\n",
       "   'joel': 289,\n",
       "   'work': 290,\n",
       "   'health': 291,\n",
       "   'services': 292,\n",
       "   'we': 293,\n",
       "   'hiring': 294,\n",
       "   'seattle': 295,\n",
       "   'wa': 296,\n",
       "   'click': 297,\n",
       "   'details': 298,\n",
       "   'nursing': 299,\n",
       "   'gt': 300,\n",
       "   'as': 301,\n",
       "   'soon': 302,\n",
       "   'maintenance': 303,\n",
       "   'ends': 304,\n",
       "   'everyone': 305,\n",
       "   'floods': 306,\n",
       "   'servers': 307,\n",
       "   'extreme': 308,\n",
       "   'starts': 309,\n",
       "   'angel': 310,\n",
       "   'arson': 311,\n",
       "   'esh': 312,\n",
       "   'please': 313,\n",
       "   'okay': 314,\n",
       "   'should': 315,\n",
       "   'fire': 316,\n",
       "   'safety': 317,\n",
       "   'rt': 318,\n",
       "   'wildfire': 319,\n",
       "   'near': 320,\n",
       "   'prepare': 321,\n",
       "   'worst': 322,\n",
       "   'men': 323,\n",
       "   'boxer': 324,\n",
       "   'curfew': 325,\n",
       "   'large': 326,\n",
       "   'ever': 327,\n",
       "   'remembered': 328,\n",
       "   'old': 329,\n",
       "   'song': 330,\n",
       "   'haven': 331,\n",
       "   'years': 332,\n",
       "   'carry': 333,\n",
       "   'memories': 334,\n",
       "   'along': 335,\n",
       "   'truth': 336,\n",
       "   '\\nhttps': 337,\n",
       "   'why': 338,\n",
       "   'would': 339,\n",
       "   'arsenal': 340,\n",
       "   'fans': 341,\n",
       "   'west': 342,\n",
       "   'battle': 343,\n",
       "   'season': 344,\n",
       "   'danger': 345,\n",
       "   'sun': 346,\n",
       "   'fun': 347,\n",
       "   'girls': 348,\n",
       "   'joe': 349,\n",
       "   'main': 350,\n",
       "   'page': 351,\n",
       "   'help': 352,\n",
       "   'survive': 353,\n",
       "   'zombie': 354,\n",
       "   'apocalypse': 355,\n",
       "   'run': 356,\n",
       "   '9th': 357,\n",
       "   'last': 358,\n",
       "   'second': 359,\n",
       "   'twister': 360,\n",
       "   'fuel': 361,\n",
       "   'line': 362,\n",
       "   'made': 363,\n",
       "   'usa': 364,\n",
       "   'favorite': 365,\n",
       "   'has': 366,\n",
       "   'drake': 367,\n",
       "   'meek': 368,\n",
       "   '@raynbowaffair': 369,\n",
       "   'editor': 370,\n",
       "   'chief': 371,\n",
       "   '@diamondkesawn': 372,\n",
       "   'releases': 373,\n",
       "   'issue': 374,\n",
       "   'ramag': 375,\n",
       "   'models': 376,\n",
       "   'mayhem': 377,\n",
       "   'wired': 378,\n",
       "   'business': 379,\n",
       "   'reddit': 380,\n",
       "   'now': 381,\n",
       "   'quarantine': 382,\n",
       "   'offensive': 383,\n",
       "   'content': 384,\n",
       "   'what': 385,\n",
       "   'progress': 386,\n",
       "   'are': 387,\n",
       "   'making': 388,\n",
       "   'middle': 389,\n",
       "   'ages': 390,\n",
       "   'books': 391,\n",
       "   'don': 392,\n",
       "   'anything': 393,\n",
       "   'nice': 394,\n",
       "   'say': 395,\n",
       "   'come': 396,\n",
       "   'sit': 397,\n",
       "   'ugh': 398,\n",
       "   'y': 399,\n",
       "   'did': 400,\n",
       "   'blaze': 401,\n",
       "   'put': 402,\n",
       "   'calories': 403,\n",
       "   'their': 404,\n",
       "   'ok': 405,\n",
       "   'cool': 406,\n",
       "   'zone': 407,\n",
       "   'war': 408,\n",
       "   'quote': 409,\n",
       "   'ps9': 410,\n",
       "   'game': 411,\n",
       "   'so': 412,\n",
       "   'u': 413,\n",
       "   'won': 414,\n",
       "   'womens': 415,\n",
       "   'buckle': 416,\n",
       "   'casual': 417,\n",
       "   'stylish': 418,\n",
       "   'handbags': 419,\n",
       "   'pockets': 420,\n",
       "   'bags': 421,\n",
       "   'green': 422,\n",
       "   'c': 423,\n",
       "   'media': 424,\n",
       "   'centre': 425,\n",
       "   'articles': 426,\n",
       "   'australia': 427,\n",
       "   'protests': 428,\n",
       "   'over': 429,\n",
       "   'security': 430,\n",
       "   'reporting': 431,\n",
       "   'blight': 432,\n",
       "   'car': 433,\n",
       "   'go': 434,\n",
       "   'ahead': 435,\n",
       "   'once': 436,\n",
       "   'mine': 437,\n",
       "   'alternatives': 438,\n",
       "   'legal': 439,\n",
       "   'rioting': 440,\n",
       "   'may': 441,\n",
       "   'most': 442,\n",
       "   'peaceful': 443,\n",
       "   'thing': 444,\n",
       "   'lil': 445,\n",
       "   'shit': 446,\n",
       "   'nearly': 447,\n",
       "   'bloody': 448,\n",
       "   'shower': 449,\n",
       "   'download': 450,\n",
       "   'itunes': 451,\n",
       "   'jazz': 452,\n",
       "   'art': 453,\n",
       "   'music': 454,\n",
       "   'still': 455,\n",
       "   'blazing': 456,\n",
       "   'looks': 457,\n",
       "   'writing': 458,\n",
       "   'computers': 459,\n",
       "   'policy': 460,\n",
       "   'goes': 461,\n",
       "   'into': 462,\n",
       "   'effect': 463,\n",
       "   'subreddits': 464,\n",
       "   'banned': 465,\n",
       "   'quarantined': 466,\n",
       "   'drop': 467,\n",
       "   'down': 468,\n",
       "   'nigga': 469,\n",
       "   'damage': 470,\n",
       "   'liked': 471,\n",
       "   'video': 472,\n",
       "   'from': 473,\n",
       "   'gun': 474,\n",
       "   'range': 475,\n",
       "   'cotton': 476,\n",
       "   'candy': 477,\n",
       "   'blizzard': 478,\n",
       "   'investigating': 479,\n",
       "   'firefighter': 480,\n",
       "   'facebook': 481,\n",
       "   'post': 482,\n",
       "   'saying': 483,\n",
       "   'police': 484,\n",
       "   'washington': 485,\n",
       "   'screaming': 486,\n",
       "   'loud': 487,\n",
       "   '@justinbieber': 488,\n",
       "   '@arianagrande': 489,\n",
       "   'hear': 490,\n",
       "   'eyewitness': 491,\n",
       "   'every': 492,\n",
       "   'kingdom': 493,\n",
       "   'against': 494,\n",
       "   'itself': 495,\n",
       "   'headed': 496,\n",
       "   'destruction': 497,\n",
       "   'house': 498,\n",
       "   'stand': 499,\n",
       "   'ancient': 500,\n",
       "   'mayan': 501,\n",
       "   'tablet': 502,\n",
       "   'hieroglyphics': 503,\n",
       "   'honors': 504,\n",
       "   'lowly': 505,\n",
       "   'king': 506,\n",
       "   '9-year-old': 507,\n",
       "   'describing': 508,\n",
       "   'hobbit': 509,\n",
       "   'smaug': 510,\n",
       "   'flames': 511,\n",
       "   'makes': 512,\n",
       "   'killer': 513,\n",
       "   'promise': 514,\n",
       "   'tax': 515,\n",
       "   'dying': 516,\n",
       "   'england': 517,\n",
       "   'east': 518,\n",
       "   'coast': 519,\n",
       "   'bank': 520,\n",
       "   'seismic': 521,\n",
       "   'v': 522,\n",
       "   'western': 523,\n",
       "   'long': 524,\n",
       "   'cable': 525,\n",
       "   'within': 526,\n",
       "   'breaking': 527,\n",
       "   'news': 528,\n",
       "   'unconfirmed': 529,\n",
       "   'heard': 530,\n",
       "   'bang': 531,\n",
       "   'nearby': 532,\n",
       "   'appears': 533,\n",
       "   'blast': 534,\n",
       "   'wind': 535,\n",
       "   'neighbour': 536,\n",
       "   'youtube': 537,\n",
       "   'minecraft': 538,\n",
       "   'night': 539,\n",
       "   'lucky': 540,\n",
       "   'block': 541,\n",
       "   'mod': 542,\n",
       "   'bob': 543,\n",
       "   'wither': 544,\n",
       "   'more': 545,\n",
       "   'see': 546,\n",
       "   'room': 547,\n",
       "   'playing': 548,\n",
       "   'yet': 549,\n",
       "   'drowning': 550,\n",
       "   'out': 551,\n",
       "   'sound': 552,\n",
       "   'ur': 553,\n",
       "   'president': 554,\n",
       "   'perfect': 555,\n",
       "   'american': 556,\n",
       "   'people': 557,\n",
       "   'win': 558,\n",
       "   'landslide': 559,\n",
       "   'demolition': 560,\n",
       "   'derby': 561,\n",
       "   'boy': 562,\n",
       "   'created': 563,\n",
       "   'mini': 564,\n",
       "   'nuclear': 565,\n",
       "   'reactor': 566,\n",
       "   'sure': 567,\n",
       "   'impact': 568,\n",
       "   'her': 569,\n",
       "   'final': 570,\n",
       "   'seen': 571,\n",
       "   'rip': 572,\n",
       "   'always': 573,\n",
       "   'open': 574,\n",
       "   'ideas': 575,\n",
       "   've': 576,\n",
       "   'one': 577,\n",
       "   'day': 578,\n",
       "   'heart': 579,\n",
       "   'gone': 580,\n",
       "   'spot': 581,\n",
       "   'flood': 582,\n",
       "   'combo': 583,\n",
       "   '9inch': 584,\n",
       "   '9w': 585,\n",
       "   'curved': 586,\n",
       "   'cree': 587,\n",
       "   'led': 588,\n",
       "   'light': 589,\n",
       "   '9x9': 590,\n",
       "   'offroad': 591,\n",
       "   'fog': 592,\n",
       "   'lamp': 593,\n",
       "   'make': 594,\n",
       "   'head': 595,\n",
       "   'cast': 596,\n",
       "   'vote': 597,\n",
       "   'jacksonville': 598,\n",
       "   'family': 599,\n",
       "   'bands': 600,\n",
       "   'together': 601,\n",
       "   'memorial': 602,\n",
       "   'planned': 603,\n",
       "   'toddler': 604,\n",
       "   'who': 605,\n",
       "   'falling': 606,\n",
       "   'hill': 607,\n",
       "   'road': 608,\n",
       "   'drought': 609,\n",
       "   'building': 610,\n",
       "   'muscle': 611,\n",
       "   'feast': 612,\n",
       "   'feeding': 613,\n",
       "   'diet': 614,\n",
       "   'plans': 615,\n",
       "   'based': 616,\n",
       "   'around': 617,\n",
       "   'losing': 618,\n",
       "   'weight': 619,\n",
       "   'sing': 620,\n",
       "   'tsunami': 621,\n",
       "   'beginners': 622,\n",
       "   'computer': 623,\n",
       "   'tutorial': 624,\n",
       "   'learn': 625,\n",
       "   'build': 626,\n",
       "   'pc': 627,\n",
       "   'governor': 628,\n",
       "   'parole': 629,\n",
       "   'california': 630,\n",
       "   'bus': 631,\n",
       "   'hijacker': 632,\n",
       "   'stay': 633,\n",
       "   'houston': 634,\n",
       "   'finnish': 635,\n",
       "   'plant': 636,\n",
       "   'move': 637,\n",
       "   '@business': 638,\n",
       "   'usually': 639,\n",
       "   'happens': 640,\n",
       "   'social': 641,\n",
       "   'mtvhottest': 642,\n",
       "   'hobo': 643,\n",
       "   'read\\x89û_': 644,\n",
       "   'about': 645,\n",
       "   'tv': 646,\n",
       "   'getting': 647,\n",
       "   'dis': 648,\n",
       "   'slowly': 649,\n",
       "   'gives': 650,\n",
       "   'pussy': 651,\n",
       "   'qew9c9m9xd': 652,\n",
       "   'view': 653,\n",
       "   'well': 654,\n",
       "   'best': 655,\n",
       "   'shot': 656,\n",
       "   'wanna': 657,\n",
       "   'leave': 658,\n",
       "   'message': 659,\n",
       "   'self': 660,\n",
       "   'detonate': 661,\n",
       "   'morgan': 662,\n",
       "   'silver': 663,\n",
       "   'dollar': 664,\n",
       "   'p': 665,\n",
       "   'gem': 666,\n",
       "   'bu': 667,\n",
       "   'ms': 668,\n",
       "   'rare': 669,\n",
       "   'hair': 670,\n",
       "   'poverty': 671,\n",
       "   'moment': 672,\n",
       "   'before': 673,\n",
       "   'weekend': 674,\n",
       "   'gets': 675,\n",
       "   'young': 676,\n",
       "   'leader': 677,\n",
       "   'straight': 678,\n",
       "   'only': 679,\n",
       "   'yourself': 680,\n",
       "   'happy': 681,\n",
       "   'fuck': 682,\n",
       "   'those': 683,\n",
       "   'tryna': 684,\n",
       "   'ruin': 685,\n",
       "   'keep': 686,\n",
       "   'feel': 687,\n",
       "   'episode': 688,\n",
       "   'got': 689,\n",
       "   'trapped': 690,\n",
       "   'almost': 691,\n",
       "   'died': 692,\n",
       "   'heat': 693,\n",
       "   'direction': 694,\n",
       "   'tell': 695,\n",
       "   'group': 696,\n",
       "   'stupid': 697,\n",
       "   'photo': 698,\n",
       "   'colorado': 699,\n",
       "   'camping': 700,\n",
       "   'let': 701,\n",
       "   'tubestrike': 702,\n",
       "   'derail': 703,\n",
       "   'mood': 704,\n",
       "   'join': 705,\n",
       "   'drinks': 706,\n",
       "   'london': 707,\n",
       "   'nfl': 708,\n",
       "   'deluge': 709,\n",
       "   'stories': 710,\n",
       "   'bored': 711,\n",
       "   'world': 712,\n",
       "   'wanting': 713,\n",
       "   'away': 714,\n",
       "   'days': 715,\n",
       "   'drown': 716,\n",
       "   'tears': 717,\n",
       "   'kinda': 718,\n",
       "   'nasty': 719,\n",
       "   'blood': 720,\n",
       "   'pun': 721,\n",
       "   'teen': 722,\n",
       "   'lonely': 723,\n",
       "   'glad': 724,\n",
       "   'survived': 725,\n",
       "   '\\n\\n': 726,\n",
       "   'being': 727,\n",
       "   'prone': 728,\n",
       "   'isn': 729,\n",
       "   'actually': 730,\n",
       "   'suffering': 731,\n",
       "   'injuries': 732,\n",
       "   'often': 733,\n",
       "   'internally': 734,\n",
       "   'wish': 735,\n",
       "   'true': 736,\n",
       "   'agree': 737,\n",
       "   'background': 738,\n",
       "   'guns': 739,\n",
       "   'weapons': 740,\n",
       "   'general': 741,\n",
       "   'sign': 742,\n",
       "   'save': 743,\n",
       "   'state': 744,\n",
       "   'actions': 745,\n",
       "   'upheaval': 746,\n",
       "   'having': 747,\n",
       "   'wounds': 748,\n",
       "   'someone': 749,\n",
       "   'doesn': 750,\n",
       "   'them': 751,\n",
       "   'disasters': 752,\n",
       "   'soul': 753,\n",
       "   'rather': 754,\n",
       "   'amazing': 755,\n",
       "   'johnson': 756,\n",
       "   'problem': 757,\n",
       "   'niggas': 758,\n",
       "   'pick': 759,\n",
       "   'fan': 760,\n",
       "   'army': 761,\n",
       "   'another': 762,\n",
       "   'crash': 763,\n",
       "   'burn': 764,\n",
       "   'maximum': 765,\n",
       "   'band': 766,\n",
       "   'obliteration': 767,\n",
       "   'crushed': 768,\n",
       "   'mile': 769,\n",
       "   'tonight': 770,\n",
       "   'awesome': 771,\n",
       "   'things': 772,\n",
       "   'seat': 773,\n",
       "   'butt': 774,\n",
       "   'desolate': 775,\n",
       "   'first': 776,\n",
       "   'tweet': 777,\n",
       "   'blown': 778,\n",
       "   'half': 779,\n",
       "   'later': 780,\n",
       "   'wrecked': 781,\n",
       "   'felt': 782,\n",
       "   'low': 783,\n",
       "   'life': 784,\n",
       "   'gopdebate': 785,\n",
       "   'explains': 786,\n",
       "   'bad': 787,\n",
       "   'debate': 788,\n",
       "   'campaign': 789,\n",
       "   '\\x89û_': 790,\n",
       "   'coastal': 791,\n",
       "   'german': 792,\n",
       "   'shepherd': 793,\n",
       "   'rescue': 794,\n",
       "   'oc': 795,\n",
       "   'shared': 796,\n",
       "   'link': 797,\n",
       "   'animalrescue': 798,\n",
       "   'does': 799,\n",
       "   'going': 800,\n",
       "   'welcome': 801,\n",
       "   'twitter': 802,\n",
       "   'find': 803,\n",
       "   'stuff': 804,\n",
       "   'amongst': 805,\n",
       "   'btw': 806,\n",
       "   'loving': 807,\n",
       "   'signed': 808,\n",
       "   'ep': 809,\n",
       "   'check': 810,\n",
       "   'salt': 811,\n",
       "   'river': 812,\n",
       "   'horses': 813,\n",
       "   'stop': 814,\n",
       "   'annihilation': 815,\n",
       "   'happen': 816,\n",
       "   'without': 817,\n",
       "   'change': 818,\n",
       "   'org': 819,\n",
       "   'thx': 820,\n",
       "   'sensor-senso': 821,\n",
       "   'standard': 822,\n",
       "   'ks9': 823,\n",
       "   'co-founder': 824,\n",
       "   'ceo': 825,\n",
       "   'steve': 826,\n",
       "   'huffman': 827,\n",
       "   'unveiled': 828,\n",
       "   'specif': 829,\n",
       "   'square': 830,\n",
       "   'write': 831,\n",
       "   'fine': 832,\n",
       "   'set': 833,\n",
       "   'ten': 834,\n",
       "   'action': 835,\n",
       "   'birth': 836,\n",
       "   'cooler': 837,\n",
       "   'jackson': 838,\n",
       "   'milkshake': 839,\n",
       "   'snowstorm': 840,\n",
       "   'had': 841,\n",
       "   'meltdown': 842,\n",
       "   'demi': 843,\n",
       "   'instagram': 844,\n",
       "   'comments': 845,\n",
       "   'grade': 846,\n",
       "   'added': 847,\n",
       "   'playlist': 848,\n",
       "   'panic': 849,\n",
       "   'disco': 850,\n",
       "   'miss': 851,\n",
       "   'ft': 852,\n",
       "   'says': 853,\n",
       "   'wild': 854,\n",
       "   'fires': 855,\n",
       "   'order': 856,\n",
       "   'canaanites': 857,\n",
       "   '@worldnetdaily': 858,\n",
       "   'running': 859,\n",
       "   'choices': 860,\n",
       "   'kid': 861,\n",
       "   'choice': 862,\n",
       "   'wear': 863,\n",
       "   'suicide': 864,\n",
       "   'vest': 865,\n",
       "   'property': 866,\n",
       "   'quiz': 867,\n",
       "   'biggest': 868,\n",
       "   'dont': 869,\n",
       "   'll': 870,\n",
       "   'transformation': 871,\n",
       "   'impossible': 872,\n",
       "   'quite': 873,\n",
       "   'medical': 874,\n",
       "   'bioterrorism': 875,\n",
       "   'sucks': 876,\n",
       "   'call': 877,\n",
       "   'duty': 878,\n",
       "   'terrorist': 879,\n",
       "   'omg': 880,\n",
       "   'remember': 881,\n",
       "   'anna': 882,\n",
       "   'horror': 883,\n",
       "   'lol': 884,\n",
       "   'freedom': 885,\n",
       "   'party': 886,\n",
       "   'summer': 887,\n",
       "   'mirage': 888,\n",
       "   'saturday': 889,\n",
       "   'tickets': 890,\n",
       "   'apparently': 891,\n",
       "   'screams': 892,\n",
       "   '\\n': 893,\n",
       "   'emotionally': 894,\n",
       "   'theres': 895,\n",
       "   'truck': 896,\n",
       "   'parking': 897,\n",
       "   'lot': 898,\n",
       "   'bitch': 899,\n",
       "   'frozen': 900,\n",
       "   'los': 901,\n",
       "   'angeles': 902,\n",
       "   'avalanche': 903,\n",
       "   'row': 904,\n",
       "   'aa': 905,\n",
       "   'grand': 906,\n",
       "   'stuart': 907,\n",
       "   'broad': 908,\n",
       "   'takes': 909,\n",
       "   'eight': 910,\n",
       "   'root': 911,\n",
       "   'runs': 912,\n",
       "   'riot': 913,\n",
       "   'aussies': 914,\n",
       "   'pair': 915,\n",
       "   'round': 916,\n",
       "   'driving': 917,\n",
       "   'videos': 918,\n",
       "   'songs': 919,\n",
       "   'hasn': 920,\n",
       "   'sunk': 921,\n",
       "   'replace': 922,\n",
       "   'inundated': 923,\n",
       "   'stands': 924,\n",
       "   'baby': 925,\n",
       "   'swim': 926,\n",
       "   'class': 927,\n",
       "   'cries': 928,\n",
       "   'entire': 929,\n",
       "   'parents': 930,\n",
       "   'evacuate': 931,\n",
       "   'students': 932,\n",
       "   'book': 933,\n",
       "   'two': 934,\n",
       "   'patrick': 935,\n",
       "   'upper': 936,\n",
       "   'red': 937,\n",
       "   'aka': 938,\n",
       "   'lots': 939,\n",
       "   'jr': 940,\n",
       "   'glass': 941,\n",
       "   'supernatural': 942,\n",
       "   'ya': 943,\n",
       "   'where': 944,\n",
       "   'magic': 945,\n",
       "   'kindle': 946,\n",
       "   'cutting': 947,\n",
       "   'posting': 948,\n",
       "   'online': 949,\n",
       "   'houses': 950,\n",
       "   'begins': 951,\n",
       "   'terrible': 952,\n",
       "   'ride': 953,\n",
       "   'till': 954,\n",
       "   'girlfriend': 955,\n",
       "   'lights': 956,\n",
       "   'again': 957,\n",
       "   'load': 958,\n",
       "   'shedding': 959,\n",
       "   'between': 960,\n",
       "   '9pm': 961,\n",
       "   'hey': 962,\n",
       "   'name': 963,\n",
       "   'gta': 964,\n",
       "   'bomber': 965,\n",
       "   'funny': 966,\n",
       "   'moments': 967,\n",
       "   'directioners': 968,\n",
       "   'fashionable': 969,\n",
       "   'mountaineering': 970,\n",
       "   'electronic': 971,\n",
       "   'watch': 972,\n",
       "   'water-resistant': 973,\n",
       "   'couples': 974,\n",
       "   'leisure': 975,\n",
       "   'tab\\x89û_': 976,\n",
       "   'place': 977,\n",
       "   'prevent': 978,\n",
       "   'perhaps': 979,\n",
       "   'fear': 980,\n",
       "   'otherwise': 981,\n",
       "   'thanks': 982,\n",
       "   '9pcs': 983,\n",
       "   'boat': 984,\n",
       "   'mining': 985,\n",
       "   '9wd': 986,\n",
       "   'beam': 987,\n",
       "   'rea\\x89û_': 988,\n",
       "   'fedex': 989,\n",
       "   'longer': 990,\n",
       "   'transport': 991,\n",
       "   'research': 992,\n",
       "   'specimens': 993,\n",
       "   'bioterror': 994,\n",
       "   'pathogens': 995,\n",
       "   'wake': 996,\n",
       "   'anthrax': 997,\n",
       "   'lab': 998,\n",
       "   'mishaps': 999,\n",
       "   ...},\n",
       "  'unk_token': '<UNK>',\n",
       "  'mask_token': '<MASK>',\n",
       "  'begin_seq_token': '<BEGIN>',\n",
       "  'end_seq_token': '<END>'},\n",
       " 'keyword_vocab': {'token_to_idx': {'trauma': 0,\n",
       "   'emergency': 1,\n",
       "   'injury': 2,\n",
       "   'body': 3,\n",
       "   'bagging': 4,\n",
       "   'windstorm': 5,\n",
       "   'collide': 6,\n",
       "   'crush': 7,\n",
       "   'detonation': 8,\n",
       "   'flattened': 9,\n",
       "   'tornado': 10,\n",
       "   'arsonist': 11,\n",
       "   'casualty': 12,\n",
       "   'lava': 13,\n",
       "   'desolation': 14,\n",
       "   'bridge': 15,\n",
       "   'collapse': 16,\n",
       "   'engulfed': 17,\n",
       "   'weapon': 18,\n",
       "   'flooding': 19,\n",
       "   'panicking': 20,\n",
       "   'sinking': 21,\n",
       "   'obliterated': 22,\n",
       "   'destroyed': 23,\n",
       "   'bag': 24,\n",
       "   'blew': 25,\n",
       "   'up': 26,\n",
       "   'demolished': 27,\n",
       "   'mass': 28,\n",
       "   'murderer': 29,\n",
       "   'traumatised': 30,\n",
       "   'famine': 31,\n",
       "   'thunder': 32,\n",
       "   'attacked': 33,\n",
       "   'collapsed': 34,\n",
       "   'buildings': 35,\n",
       "   'burning': 36,\n",
       "   'police': 37,\n",
       "   'aftershock': 38,\n",
       "   'services': 39,\n",
       "   'floods': 40,\n",
       "   'arson': 41,\n",
       "   'tsunami': 42,\n",
       "   'wildfire': 43,\n",
       "   'curfew': 44,\n",
       "   'terrorism': 45,\n",
       "   'danger': 46,\n",
       "   'derailed': 47,\n",
       "   'survive': 48,\n",
       "   'twister': 49,\n",
       "   'mayhem': 50,\n",
       "   'quarantine': 51,\n",
       "   'burned': 52,\n",
       "   'blaze': 53,\n",
       "   'war': 54,\n",
       "   'zone': 55,\n",
       "   'bomb': 56,\n",
       "   'bags': 57,\n",
       "   'refugees': 58,\n",
       "   'blight': 59,\n",
       "   'rioting': 60,\n",
       "   'blazing': 61,\n",
       "   'quarantined': 62,\n",
       "   'damage': 63,\n",
       "   'blizzard': 64,\n",
       "   'screaming': 65,\n",
       "   'eyewitness': 66,\n",
       "   'destruction': 67,\n",
       "   'upheaval': 68,\n",
       "   'flames': 69,\n",
       "   'fire': 70,\n",
       "   'seismic': 71,\n",
       "   'loud': 72,\n",
       "   'bang': 73,\n",
       "   'apocalypse': 74,\n",
       "   'drowning': 75,\n",
       "   'landslide': 76,\n",
       "   'demolition': 77,\n",
       "   'nuclear': 78,\n",
       "   'reactor': 79,\n",
       "   'flood': 80,\n",
       "   'battle': 81,\n",
       "   'drowned': 82,\n",
       "   'drought': 83,\n",
       "   'hijacker': 84,\n",
       "   'hurricane': 85,\n",
       "   'fatality': 86,\n",
       "   'detonate': 87,\n",
       "   'ruin': 88,\n",
       "   'trapped': 89,\n",
       "   'deluge': 90,\n",
       "   'derail': 91,\n",
       "   'drown': 92,\n",
       "   'blood': 93,\n",
       "   'survived': 94,\n",
       "   'injuries': 95,\n",
       "   'weapons': 96,\n",
       "   'annihilation': 97,\n",
       "   'wounds': 98,\n",
       "   'army': 99,\n",
       "   'crash': 100,\n",
       "   'obliteration': 101,\n",
       "   'crushed': 102,\n",
       "   'blown': 103,\n",
       "   'desolate': 104,\n",
       "   'wrecked': 105,\n",
       "   'rescue': 106,\n",
       "   'on': 107,\n",
       "   'snowstorm': 108,\n",
       "   'meltdown': 109,\n",
       "   'panic': 110,\n",
       "   'wild': 111,\n",
       "   'fires': 112,\n",
       "   'suicide': 113,\n",
       "   'bioterrorism': 114,\n",
       "   'terrorist': 115,\n",
       "   'pandemonium': 116,\n",
       "   'screams': 117,\n",
       "   'truck': 118,\n",
       "   'avalanche': 119,\n",
       "   'riot': 120,\n",
       "   'sunk': 121,\n",
       "   'inundated': 122,\n",
       "   'evacuate': 123,\n",
       "   'bloody': 124,\n",
       "   'bomber': 125,\n",
       "   'fear': 126,\n",
       "   'bioterror': 127,\n",
       "   'demolish': 128,\n",
       "   'screamed': 129,\n",
       "   'devastated': 130,\n",
       "   'bleeding': 131,\n",
       "   'dead': 132,\n",
       "   'explode': 133,\n",
       "   'storm': 134,\n",
       "   'massacre': 135,\n",
       "   'siren': 136,\n",
       "   'cliff': 137,\n",
       "   'fall': 138,\n",
       "   'military': 139,\n",
       "   'sirens': 140,\n",
       "   'armageddon': 141,\n",
       "   'hazardous': 142,\n",
       "   'murder': 143,\n",
       "   'heat': 144,\n",
       "   'wave': 145,\n",
       "   'hostage': 146,\n",
       "   'hellfire': 147,\n",
       "   'ambulance': 148,\n",
       "   'stretcher': 149,\n",
       "   'bombed': 150,\n",
       "   'harm': 151,\n",
       "   'bombing': 152,\n",
       "   'electrocute': 153,\n",
       "   'exploded': 154,\n",
       "   'mudslide': 155,\n",
       "   'explosion': 156,\n",
       "   'hostages': 157,\n",
       "   'natural': 158,\n",
       "   'disaster': 159,\n",
       "   'wreck': 160,\n",
       "   'catastrophe': 161,\n",
       "   'smoke': 162,\n",
       "   'deaths': 163,\n",
       "   'airplane': 164,\n",
       "   'accident': 165,\n",
       "   'whirlwind': 166,\n",
       "   'attack': 167,\n",
       "   'dust': 168,\n",
       "   'fatal': 169,\n",
       "   'catastrophic': 170,\n",
       "   'ablaze': 171,\n",
       "   'collided': 172,\n",
       "   '_nan_': 173,\n",
       "   'cyclone': 174,\n",
       "   'deluged': 175,\n",
       "   'annihilated': 176,\n",
       "   'destroy': 177,\n",
       "   'chemical': 178,\n",
       "   'rainstorm': 179,\n",
       "   'displaced': 180,\n",
       "   'hazard': 181,\n",
       "   'rescued': 182,\n",
       "   'thunderstorm': 183,\n",
       "   'death': 184,\n",
       "   'sandstorm': 185,\n",
       "   'first': 186,\n",
       "   'responders': 187,\n",
       "   'trouble': 188,\n",
       "   'evacuation': 189,\n",
       "   'tragedy': 190,\n",
       "   'lightning': 191,\n",
       "   'hailstorm': 192,\n",
       "   'hijacking': 193,\n",
       "   'devastation': 194,\n",
       "   'obliterate': 195,\n",
       "   'hijack': 196,\n",
       "   'bush': 197,\n",
       "   'plan': 198,\n",
       "   'earthquake': 199,\n",
       "   'forest': 200,\n",
       "   'injured': 201,\n",
       "   'threat': 202,\n",
       "   'rubble': 203,\n",
       "   'structural': 204,\n",
       "   'failure': 205,\n",
       "   'fatalities': 206,\n",
       "   'survivors': 207,\n",
       "   'inundation': 208,\n",
       "   'volcano': 209,\n",
       "   'collision': 210,\n",
       "   'radiation': 211,\n",
       "   'crashed': 212,\n",
       "   'hail': 213,\n",
       "   'epicentre': 214,\n",
       "   'evacuated': 215,\n",
       "   'electrocuted': 216,\n",
       "   'razed': 217,\n",
       "   'casualties': 218,\n",
       "   'sinkhole': 219,\n",
       "   'oil': 220,\n",
       "   'spill': 221,\n",
       "   'wounded': 222,\n",
       "   'violent': 223,\n",
       "   'rescuers': 224,\n",
       "   'typhoon': 225,\n",
       "   'outbreak': 226,\n",
       "   'wreckage': 227,\n",
       "   'derailment': 228,\n",
       "   'debris': 229}}}"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vect=TweetVectorizer.from_dataframe(train_splits)\n",
    "test_vect.to_serializable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e655989",
   "metadata": {},
   "source": [
    "The Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "166ff418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweet_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tweet_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (TweetVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.tweet_df = tweet_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        def measure_len(text):\n",
    "            len_=0\n",
    "            for word in text:\n",
    "                if word not in string.punctuation:\n",
    "                    len_+=1\n",
    "            return len_\n",
    "                    \n",
    "        #measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, tweet_df.text)) + 2\n",
    "        \n",
    "\n",
    "        self.train_df = self.tweet_df[self.tweet_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.tweet_df[self.tweet_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = tweet_df.target.value_counts().to_dict()\n",
    "        freq = [count for _, count in class_counts.items()]\n",
    "        self.class_weights = 1.0 / torch.tensor(freq, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, tweet_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            tweet_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of TweetDataset\n",
    "        \"\"\"\n",
    "        tweet_df = pd.read_csv(tweet_csv)\n",
    "        train_tweet_df = tweet_df[tweet_df.split=='train']\n",
    "        return cls(tweet_df, TweetVectorizer.from_dataframe(train_tweet_df))\n",
    "\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        text_vector = \\\n",
    "            self._vectorizer.vectorize_text(row.text, self._max_seq_length)\n",
    "\n",
    "        keyword_vector = \\\n",
    "            self._vectorizer.vectorize_keyword(row.keyword)\n",
    "        \n",
    "        \n",
    "        return {'x_data': text_vector,\n",
    "                'keyword_vector': keyword_vector,                \n",
    "                'y_target': row.target}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "f2dc8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=TweetDataset.load_dataset_and_make_vectorizer(args.train_split_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "eb77659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_data': array([  2,   1, 515, 516, 517,  42,  87, 518,   7, 476, 519, 520, 193,\n",
       "        376, 521,   8,  79,   7, 522,  48, 523, 462,  61, 524,  29, 263,\n",
       "          3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0], dtype=int64),\n",
       " 'keyword_vector': array([72, 73,  0], dtype=int64),\n",
       " 'y_target': 0}"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "99c9eaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('body bags',\n",
       " \" ' your body will heal the bags under your eyes will go away you\\x89ûªll be so happy you\\x89ûªll smile and really .  .  .  '  http :  /  / t . co / wukcalnqms\")"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_splits.iloc[236].keyword, train_splits.iloc[236].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "b75c5a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1484</td>\n",
       "      <td>body bags</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>womens buckle casual stylish shoulder handbags...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1526</td>\n",
       "      <td>body bags</td>\n",
       "      <td>Speaking the Truth in Love</td>\n",
       "      <td>fairfax investigating firefighter over faceboo...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1519</td>\n",
       "      <td>body bags</td>\n",
       "      <td>NaN</td>\n",
       "      <td>' your body will heal the bags under your eye...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1520</td>\n",
       "      <td>body bags</td>\n",
       "      <td>California, USA</td>\n",
       "      <td>womens handbags cross body geometric pattern s...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1486</td>\n",
       "      <td>body bags</td>\n",
       "      <td>CA</td>\n",
       "      <td>bestseller !  fossil dawson mini cross body ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    keyword                    location  \\\n",
       "55   1484  body bags              United Kingdom   \n",
       "68   1526  body bags  Speaking the Truth in Love   \n",
       "236  1519  body bags                         NaN   \n",
       "348  1520  body bags             California, USA   \n",
       "586  1486  body bags                          CA   \n",
       "\n",
       "                                                  text  target  split  \n",
       "55   womens buckle casual stylish shoulder handbags...       0  train  \n",
       "68   fairfax investigating firefighter over faceboo...       0  train  \n",
       "236   ' your body will heal the bags under your eye...       0  train  \n",
       "348  womens handbags cross body geometric pattern s...       0  train  \n",
       "586  bestseller !  fossil dawson mini cross body ba...       0  train  "
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_splits[train_splits.keyword=='body bags'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b8ad6",
   "metadata": {},
   "source": [
    "As we can see, the keyword often does not reflect the context of the message at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "1bc397d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"A generator function which wraps the PyTorch DataLoader. \n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6df341",
   "metadata": {},
   "source": [
    "The Model: RNNClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "632b892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, rnn_hidden_size, \n",
    "                 dropout_p, pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors (the size of each embedding vector)\n",
    "            num_embeddings (int): number of embedding vectors(size of the dictionary of embeddings)            \n",
    "            rnn_hidden_size (int): the size of the hidden dimension in GRU            \n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                    embedding_dim=embedding_size,                                    \n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                    embedding_dim=embedding_size,                                    \n",
    "                                    padding_idx=padding_idx,\n",
    "                                    _weight=pretrained_embeddings)\n",
    "            \n",
    "        self.rnn = nn.GRU(input_size=embedding_size, hidden_size=rnn_hidden_size, batch_first=True, bidirectional=True)            \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size*2, out_features=1)\n",
    "        self._dropout_p = dropout_p\n",
    "        #self._relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \n",
    "        x_embedded = self.emb(x_in)\n",
    "        y_all, y_end = self.rnn(x_embedded)\n",
    "        batch_size, seq_size, feat_size = y_all.shape\n",
    "        y_all = y_all.contiguous().view(batch_size * seq_size, feat_size)\n",
    "        #y_all=self._relu(y_all)\n",
    "        y_all = self.fc(F.dropout(y_all, p=self._dropout_p))        \n",
    "                               \n",
    "        if apply_sigmoid:\n",
    "            y_all = F.sigmoid(y_all)\n",
    "            \n",
    "        y_all = y_all.contiguous().view(batch_size, 1, seq_size)\n",
    "        \n",
    "        y_pred=F.avg_pool1d(y_all, seq_size).squeeze(dim=2)        \n",
    "        \n",
    "       \n",
    "        return y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "5cf58722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "4604648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    #with open(glove_filepath, \"r\") as fp:\n",
    "    with io.open(glove_filepath, encoding='utf-8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "bc167599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_score': [],\n",
    "            'val_loss': [],\n",
    "            'val_score': [],\n",
    "            'test_loss': -1,\n",
    "            'test_score': -1,\n",
    "            'model_filename': args.model_state_file}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a66a6",
   "metadata": {},
   "source": [
    "Settings and some prep work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "c82dcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_dataset_csv=\"train.csv\",\n",
    "    test_dataset_csv=\"test.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.3,\n",
    "    train_split_csv=\"train_with_splits.csv\", \n",
    "    #train_split_csv=\"train_tweet.csv\",\n",
    "    glove_patch='C:/Users/Admin/Desktop/NLP/embed/glove.6B.100d.txt',\n",
    "    model_state_file=\"model.pth\",\n",
    "    seed=123,\n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    rnn_hidden_size=100,     \n",
    "    # Training hyper parameter    \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.3, \n",
    "    batch_size=128, \n",
    "    num_epochs=30, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True\n",
    "   \n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "18cfa84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247f14a",
   "metadata": {},
   "source": [
    "Initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "f71429f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_glove = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "90ff301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# create dataset and vectorizer\n",
    "dataset = TweetDataset.load_dataset_and_make_vectorizer(args.train_split_csv)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.text_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_patch, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "e97f9156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3404, 100)"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "f687de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (emb): Embedding(3404, 100, padding_idx=0)\n",
       "  (rnn): GRU(100, 100, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RNNClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.text_vocab),                            \n",
    "                            rnn_hidden_size=args.rnn_hidden_size,                            \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            #pretrained_embeddings=None,\n",
    "                            padding_idx=0)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "7cf9d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(y_target, y_pred):\n",
    "    y_target_nump = y_target.cpu().bool().numpy()\n",
    "    y_pred_nump = (torch.sigmoid(y_pred)>0.5).cpu().numpy()\n",
    "    #y_pred_nump = (y_pred>0.5).cpu().long().numpy()\n",
    "    if y_target_nump.sum()==0:\n",
    "        zv=1\n",
    "    else:\n",
    "        zv=0   \n",
    "    prec=precision(y_target_nump, y_pred_nump, zero_division=zv)\n",
    "    rec=recall(y_target_nump, y_pred_nump, zero_division=zv) \n",
    "    div=(prec+rec)\n",
    "    if div!=0:\n",
    "        return 2*prec*rec/div\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef339e",
   "metadata": {},
   "source": [
    "Training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "d4d504d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b074e6db95f4ae6993594c3be62b59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb89175123f040af8c115c416f6e5f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6458bbbe3ddf471989d18ef31cd6eda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                        total=args.num_epochs,\n",
    "                        position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                    total=dataset.get_num_batches(args.batch_size), \n",
    "                    position=1, \n",
    "                    leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_score = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].long())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the score\n",
    "            score_t = compute_score(batch_dict['y_target'], y_pred)\n",
    "            running_score += (score_t - running_score) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                  score=running_score, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_score'].append(running_score)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_score = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].long())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the score\n",
    "            #print(prec_(batch_dict['y_target'], y_pred))\n",
    "            score_t = compute_score(batch_dict['y_target'], y_pred)\n",
    "            running_score += (score_t - running_score) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                score=running_score, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_score'].append(running_score)\n",
    "       \n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "270666aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.38571786463150237,\n",
       " 0.3785239620119938,\n",
       " 0.6853752515464574,\n",
       " 0.6622662408696478,\n",
       " 0.7343274310651297,\n",
       " 0.7469441251243459,\n",
       " 0.7220251421271727,\n",
       " 0.7613986486868969,\n",
       " 0.7517713073479426,\n",
       " 0.7586485139622124,\n",
       " 0.7532694206741706,\n",
       " 0.7339676334027201,\n",
       " 0.7598604250297295,\n",
       " 0.7627357478267159,\n",
       " 0.7597114833987981,\n",
       " 0.7627883089535341,\n",
       " 0.7617852697446603,\n",
       " 0.7630845183584016,\n",
       " 0.7590623085455817,\n",
       " 0.7581127562174875,\n",
       " 0.7623577775482678,\n",
       " 0.7663495672192375,\n",
       " 0.7616477787654382,\n",
       " 0.7662441533915714,\n",
       " 0.7609197375700202,\n",
       " 0.7631577775146676,\n",
       " 0.7647830071963077,\n",
       " 0.7605370240235659,\n",
       " 0.7611922004723347]"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state['val_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af3a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
